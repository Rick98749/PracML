---
title: "Practical ML peer assignment"
author: "Suryadipta Das"
date: "October 23, 2018"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background/Purpose

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. 

## Loading required libraries

```{r}
library(caret)
library(rpart)
library(ggplot2)
library(corrplot)
library(randomForest)
library(rattle)
```

## Loading the data from given websites and checking the dimensions

```{r}
train_raw <- read.csv("C:/Users/Rick/Documents/pml-training.csv")[,-1]
test <- read.csv("C:/Users/Rick/Documents/pml-testing.csv")[,-1]
dim(train_raw)
dim(test)
```

## Cleaning the dataset for analysis

```{r}
Zero <- nearZeroVar(train_raw)
training <- train_raw[, -Zero]
testing <- test[, -Zero]

NaValues <- sapply(training, function(x) mean(is.na(x))) > 0.9
training <- training[, NaValues == "FALSE"]
testing <- testing[, NaValues == "FALSE"]

training <- training[,-c(1:5)]
testing <- testing[,-c(1:5)]

```

## Getting an idea about the clean dataset

```{r}
summary(training)
summary(testing)
head(training)

```

## Splitting data into train and test sets
```{r}
inTrain <- createDataPartition(y= training$classe, p = 0.7, list = FALSE)
training <- training[inTrain, ]
crossvalidation <- training[-inTrain, ]
```

## Training our model with Principal Component Analysis
```{r}
tree <- train(classe~., data = training, method = "rpart")

predict_training_tree <- predict(tree, training)
confusionmatrix_training_tree <- confusionMatrix(predict_training_tree, training$classe)

predict_crossvalidation_tree <- predict(tree, crossvalidation)
confusionmatrix_cv_tree <- confusionMatrix(predict_crossvalidation_tree, crossvalidation$classe)

print(confusionmatrix_cv_tree)

```

## Training our model with random forest
```{r}
random <- train(classe~., data = training, method = "rf")

predict_training_rf <- predict(random, training)
confusionmatrix_training_rf <- confusionMatrix(predict_training_rf, training$classe)

predict_crossvalidation_rf <- predict(random, crossvalidation)
confusionmatrix_cv_rf <- confusionMatrix(predict_crossvalidation_rf, crossvalidation$classe)

print(confusionmatrix_cv_rf)

```

## Conclusion:
We see that the accuracy of the random forest model is more than that of PCA. Hence random forest is preferred for training this dataset.

## Checking on the cross validation set

```{r}
predict_testing <- predict(random, testing)
predict_testing

```